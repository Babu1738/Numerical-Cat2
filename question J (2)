def qr_algorithm(A, num_simulations: int):
    Ak = A.copy()
    Q_prod = np.eye(A.shape[0])

    for _ in range(num_simulations):
        Q, R = np.linalg.qr(Ak)
        Ak = np.dot(R, Q)
        Q_prod = np.dot(Q_prod, Q)
    
    eigenvalues = np.diag(Ak)
    eigenvectors = Q_prod
    
    return eigenvalues, eigenvectors

# Define the matrix
A = np.array([[4, 1, 1], 
              [1, 3, -1], 
              [1, -1, 2]])

eigenvalues, eigenvectors = qr_algorithm(A, 100)
print("Eigenvalues:", eigenvalues)
print("Eigenvectors:")
print(eigenvectors)

k)
import matplotlib.pyplot as plt
import numpy as np

def f(x, y):
    return x**2 + y**2 - x*y + x - y + 1

def grad_f(x, y):
    df_dx = 2*x - y + 1
    df_dy = 2*y - x - 1
    return [df_dx, df_dy]

def gradient_descent(starting_point, learning_rate, num_iterations):
    point = list(starting_point)
    points = [point.copy()]
    
    for _ in range(num_iterations):
        grad = grad_f(point[0], point[1])
        point[0] -= learning_rate * grad[0]
        point[1] -= learning_rate * grad[1]
        points.append(point.copy())
    
    return points

# Parameters
starting_point = [0, 0]
learning_rate = 0.1
num_iterations = 100

# Perform Gradient Descent
points = gradient_descent(starting_point, learning_rate, num_iterations)

# Extract points for plotting
x_points = [point[0] for point in points]
y_points = [point[1] for point in points]

# Create a grid of x and y values
x = np.linspace(-1, 2, 400)
y = np.linspace(-1, 2, 400)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# Plot the results
plt.contour(X, Y, Z, levels=50)
plt.plot(x_points, y_points, 'r.-')
plt.title('Gradient Descent Optimization')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
